{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are 4 sizes that are used as type parameters for the needed protocols.\n",
    "## using the `Dim` class, we can ensure that our objects align in their dimensionalities.\n",
    "\n",
    "## C: The number of conditional variables\n",
    "\n",
    "from modugant.matrix.dim import Dim\n",
    "\n",
    "conditions = Dim[0](0)\n",
    "## L: The number of latent variables\n",
    "latent = Dim[10](10)\n",
    "## G: The number of generated features\n",
    "generated = Dim[5](5)\n",
    "## D: The number of real features to discriminate\n",
    "dim = Dim[5](5)\n",
    "## The batch size\n",
    "batch = Dim[8](8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4984, 0.4999, 0.4512, 0.3986, 0.5031],\n",
      "        [0.4985, 0.4938, 0.4426, 0.3993, 0.5048],\n",
      "        [0.4982, 0.4945, 0.4427, 0.3994, 0.5049],\n",
      "        [0.5047, 0.4912, 0.4405, 0.4068, 0.4965],\n",
      "        [0.4983, 0.4972, 0.4468, 0.3991, 0.5040],\n",
      "        [0.4982, 0.4945, 0.4427, 0.3994, 0.5049],\n",
      "        [0.4989, 0.4972, 0.4479, 0.3991, 0.5033],\n",
      "        [0.5055, 0.4893, 0.4374, 0.4085, 0.4957]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## A generator maps `(C, L) -> (G)`\n",
    "\n",
    "## Instatiate a sequential generator\n",
    "\n",
    "from modugant.generators import SequentialGenerator\n",
    "from modugant.matrix import Matrix\n",
    "from modugant.matrix.ops import zeros\n",
    "\n",
    "conditions = Dim[0](0)\n",
    "latent = Dim[10](10)\n",
    "generated = Dim[5](5)\n",
    "batch = Dim[8](8)\n",
    "\n",
    "generator = SequentialGenerator(\n",
    "    conditions,\n",
    "    latent,\n",
    "    generated,\n",
    "    steps = [10, 10, 5],\n",
    "    learning = 0.01,\n",
    "    gamma = 0.1,\n",
    "    step = 100\n",
    ")\n",
    "\n",
    "## The generator can be used to sample from the latent space\n",
    "## Pass in a 0-width condition of the desired length\n",
    "## Use `Matrix.zeros` for type/dimension alignment\n",
    "print(generator.sample(zeros((batch, conditions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6909,  0.2056,  0.8441, -0.9408,  0.2882],\n",
      "        [ 0.6643,  0.3398, -0.3775,  0.4350, -0.4729],\n",
      "        [-0.3590, -0.0150,  0.4493, -0.5246, -0.2024],\n",
      "        [ 0.3525,  0.3371,  0.1595, -0.0891, -0.3798],\n",
      "        [ 0.4280,  0.2691,  0.1975, -0.4721, -0.1675],\n",
      "        [ 0.5172, -0.0845, -0.4411,  0.6337, -0.3657],\n",
      "        [-0.2107,  0.7293,  0.1717, -0.3085,  0.1956],\n",
      "        [ 0.1047,  0.3106,  0.4389, -0.5143, -0.1937]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Instantiate a residual generator\n",
    "\n",
    "from modugant.generators import ResidualGenerator\n",
    "\n",
    "conditions = Dim[0](0)\n",
    "latent = Dim[10](10)\n",
    "generated = Dim[5](5)\n",
    "batch = Dim[8](8)\n",
    "\n",
    "generator = ResidualGenerator(\n",
    "    conditions,\n",
    "    latent,\n",
    "    generated,\n",
    "    steps = [10, 10, 5],\n",
    "    learning = 0.01,\n",
    "    decay = 0.001\n",
    ")\n",
    "\n",
    "print(generator.sample(zeros((batch, conditions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4077,  0.2677,  0.3967,  0.0790,  0.2311],\n",
      "        [ 0.3939,  0.3259,  0.1252,  0.0035, -0.3723],\n",
      "        [ 0.2634, -1.0318, -0.7844,  0.7270, -0.3338],\n",
      "        [ 0.3618,  0.5002,  0.9131,  0.2635,  0.0556],\n",
      "        [ 0.0288,  1.2959, -0.6457, -0.0047,  0.2984],\n",
      "        [ 0.3421,  0.2940, -0.5743,  0.3495, -0.7437],\n",
      "        [-0.9578,  0.5221, -0.1214,  1.5171,  0.9110],\n",
      "        [ 0.3485,  0.2320,  0.4842,  0.4727, -0.1676]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Create a custom generator that fully implements the `Generator` protocol\n",
    "\n",
    "from typing import Self, cast, override\n",
    "\n",
    "from torch import Tensor, no_grad\n",
    "from torch.nn import Linear, Module\n",
    "from torch.optim.adam import Adam\n",
    "\n",
    "from modugant import Generator\n",
    "from modugant.device import Device\n",
    "from modugant.matrix.dim import One\n",
    "from modugant.matrix.ops import cat, normal\n",
    "\n",
    "\n",
    "class CustomGenerator[L: int, G: int](Module, Generator[One, L, G]):\n",
    "    def __init__(self, latent: L, generated: G):\n",
    "        super().__init__()\n",
    "        self._conditions = Dim.one()\n",
    "        self._latents = latent\n",
    "        self._intermediates = generated\n",
    "        self._model = Linear(latent + 1, generated)\n",
    "        self._optimizer = Adam(self.parameters(), lr = 0.01)\n",
    "    @override\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._model(x)\n",
    "    @override\n",
    "    def sample[N: int](self, condition: Matrix[N, One]) -> Matrix[N, G]:\n",
    "        ## Make sure not to set device, it will be controlled by context!!\n",
    "        noise = normal(0, 1, (condition.shape[0], self._latents))\n",
    "        inputs = cat((condition, noise), dim = 1, shape = (condition.shape[0], self._latents + 1))\n",
    "        outputs = self.forward(inputs)\n",
    "        return Matrix.load(outputs, shape = (condition.shape[0], self._intermediates))\n",
    "    @override\n",
    "    def update(self, loss: Tensor) -> None:\n",
    "        self.zero_grad()\n",
    "        _ = loss.backward()\n",
    "        cast(None, self._optimizer.step())\n",
    "    @override\n",
    "    def reset(self) -> None:\n",
    "        with no_grad():\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, Linear):\n",
    "                    module.reset_parameters()\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = 0.01)\n",
    "    @override\n",
    "    def move(self, device: Device) -> Self:\n",
    "        return self.to(device)\n",
    "    @override\n",
    "    def train(self, mode: bool = True) -> Self:\n",
    "        return self.train(mode)\n",
    "    @property\n",
    "    @override\n",
    "    def rate(self) -> float:\n",
    "        return self._optimizer.param_groups[0]['lr']\n",
    "\n",
    "latent = Dim[10](10)\n",
    "generated = Dim[5](5)\n",
    "batch = Dim[8](8)\n",
    "\n",
    "generator = CustomGenerator(latent, generated)\n",
    "\n",
    "print(generator.sample(normal(0, 1, (batch, Dim.one()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2854,  1.1247,  1.7791, -1.0458,  0.0507],\n",
      "        [-0.2544,  0.7251,  1.6548, -0.0276, -0.7635],\n",
      "        [ 0.0733,  0.0490, -0.0177, -0.8428, -0.1150],\n",
      "        [-0.1081,  0.0540, -0.7972, -0.2182,  0.5202],\n",
      "        [ 0.2779, -0.4938,  0.0600, -0.5387, -0.0948],\n",
      "        [-0.3534,  0.7919,  0.8774, -0.9429, -0.6382],\n",
      "        [-0.5599,  1.0466,  0.9090, -1.0613, -0.0298],\n",
      "        [ 0.2569,  0.5898,  0.1190, -0.4355,  0.3592]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Create a custom generator by extending `BasicGenerator` abstract class\n",
    "\n",
    "from modugant.generators import BasicGenerator\n",
    "\n",
    "\n",
    "class CustomGenerator[L: int, G: int](BasicGenerator[One, L, G]):\n",
    "    def __init__(self, latent: L, generated: G):\n",
    "        super().__init__(Dim.one(), latent, generated)\n",
    "        self._model = Linear(latent + 1, generated)\n",
    "        self._optimizer = Adam(self.parameters(), lr = 0.01)\n",
    "    @override\n",
    "    def _latent[N: int](self, batch: N) -> Matrix[N, L]:\n",
    "        return normal(0, 1, (batch, self._latents))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = 0.01)\n",
    "\n",
    "latent = Dim[10](10)\n",
    "generated = Dim[5](5)\n",
    "batch = Dim[8](8)\n",
    "\n",
    "generator = CustomGenerator(latent, generated)\n",
    "\n",
    "print(generator.sample(normal(0, 1, (batch, Dim.one()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a scheduler to the generator\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "class ScheduledGenerator[L: int, G: int](CustomGenerator[L, G]):\n",
    "    def __init__(self, latent: L, generated: G):\n",
    "        super().__init__(latent, generated)\n",
    "        self._scheduler = StepLR(self._optimizer, step_size = 100, gamma = 0.1)\n",
    "    @override\n",
    "    def update(self, loss: Matrix[One, One]) -> None:\n",
    "        super().update(loss)\n",
    "        self._scheduler.step()\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        super().restart()\n",
    "        self._scheduler = StepLR(self._optimizer, step_size = 100, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
