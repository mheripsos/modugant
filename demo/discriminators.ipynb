{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are 4 sizes that are used as type parameters for the needed protocols.\n",
    "## using the `Dim` class, we can ensure that our objects align in their dimensionalities.\n",
    "\n",
    "## C: The number of conditional variables\n",
    "\n",
    "from modugant.matrix.dim import Dim\n",
    "\n",
    "conditions = Dim[0](0)\n",
    "## L: The number of latent variables\n",
    "latent = Dim[10](10)\n",
    "## G: The number of generated features\n",
    "generated = Dim[5](5)\n",
    "## D: The number of real features to discriminate\n",
    "dim = Dim[5](5)\n",
    "## The batch size\n",
    "batch = Dim[8](8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5201],\n",
      "        [0.5213],\n",
      "        [0.5172],\n",
      "        [0.5458],\n",
      "        [0.5041],\n",
      "        [0.5026],\n",
      "        [0.4996],\n",
      "        [0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7095]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implement the `Discriminator` protocol\n",
    "\n",
    "from typing import Self, cast, override\n",
    "\n",
    "from torch import Tensor\n",
    "from torch import cat as t_cat\n",
    "from torch.nn import Dropout, Linear, Module, ReLU, Sequential, Sigmoid\n",
    "from torch.optim import Adam\n",
    "\n",
    "from modugant.device import Device\n",
    "from modugant.matrix import Matrix\n",
    "from modugant.matrix.dim import One\n",
    "from modugant.matrix.ops import rand, randn, zeros\n",
    "from modugant.protocols import Discriminator\n",
    "\n",
    "\n",
    "class IrisDiscriminator[C: int, D: int](Module, Discriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._conditions = conditions\n",
    "        self._outputs = outputs\n",
    "        self.__lr = lr\n",
    "        self._model = Sequential(\n",
    "            *[\n",
    "                Sequential(\n",
    "                    Linear(\n",
    "                        steps[i - 1] if i else outputs + conditions,\n",
    "                        steps[i]\n",
    "                    ),\n",
    "                    ReLU(),\n",
    "                    Dropout(0.2)\n",
    "                )\n",
    "                for i in range(len(steps))\n",
    "            ],\n",
    "            Linear(\n",
    "                steps[-1] if len(steps) else outputs + conditions,\n",
    "                1\n",
    "            ),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._model(x)\n",
    "    @override\n",
    "    def predict[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Matrix[N, One]:\n",
    "        return cast(Matrix[N, One], self.forward(t_cat([condition, data], dim = 1)))\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def step[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        self.zero_grad()\n",
    "        loss = self.loss(condition, data, target)\n",
    "        _ = loss.backward()\n",
    "        cast(None, self._optimizer.step())\n",
    "        return loss\n",
    "    @override\n",
    "    def reset(self) -> None:\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Linear):\n",
    "                module.reset_parameters()\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "    @override\n",
    "    def move(self, device: Device) -> Self:\n",
    "        return self.to(device)\n",
    "    @override\n",
    "    def train(self, mode: bool = True) -> Self:\n",
    "        return super().train(mode)\n",
    "    @property\n",
    "    @override\n",
    "    def rate(self) -> float:\n",
    "        return self._optimizer.param_groups[0]['lr']\n",
    "\n",
    "discriminator = IrisDiscriminator(conditions, dim, [10, 10, 5], 0.01)\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5127],\n",
      "        [0.4763],\n",
      "        [0.5033],\n",
      "        [0.5000],\n",
      "        [0.5102],\n",
      "        [0.4621],\n",
      "        [0.5168],\n",
      "        [0.4551]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6818]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use a `BasicDiscriminator` to implement `IrisDiscriminator` with the same functionality\n",
    "\n",
    "from modugant.discriminators import BasicDiscriminator\n",
    "\n",
    "# assign a ._model and ._optimizer, and implement loss() and restart()\n",
    "# our previous implementations of the other methods are identical in `BasicDiscriminator`\n",
    "\n",
    "class IrisDiscriminator[C: int, D: int](BasicDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(conditions, outputs)\n",
    "        self.__lr = lr\n",
    "        self._model = Sequential(\n",
    "            *[\n",
    "                Sequential(\n",
    "                    Linear(\n",
    "                        steps[i - 1] if i else outputs + conditions,\n",
    "                        steps[i]\n",
    "                    ),\n",
    "                    ReLU(),\n",
    "                    Dropout(0.2)\n",
    "                )\n",
    "                for i in range(len(steps))\n",
    "            ],\n",
    "            Linear(\n",
    "                steps[-1] if len(steps) else outputs + conditions,\n",
    "                1\n",
    "            ),\n",
    "            Sigmoid()\n",
    "        )\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "\n",
    "discriminator = IrisDiscriminator(conditions, dim, [10, 10, 5], 0.01)\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5742],\n",
      "        [0.5887],\n",
      "        [0.5981],\n",
      "        [0.5853],\n",
      "        [0.5882],\n",
      "        [0.5957],\n",
      "        [0.5885],\n",
      "        [0.6146]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6514]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use an `ExtendedDiscriminator` to override any method of an existing discriminator class\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from modugant.discriminators.extended import ExtendedDiscriminator\n",
    "\n",
    "# Create a container class that adds a scheduler to a discriminator\n",
    "# Note that this could have been done directly in the `IrisDiscriminator` class\n",
    "# But we can create a classes with and without the scheduler, but otherwise identical\n",
    "\n",
    "class ScheduledDiscriminator[C: int, D: int](ExtendedDiscriminator[C, D]):\n",
    "    ## Narrow the expected type to the BasicDiscriminator protocol\n",
    "    ## This ensures that there is a .optimizer for us to schedule\n",
    "    _discriminator: BasicDiscriminator[C, D]\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator: BasicDiscriminator[C, D], ## Narrow the expected type to the BasicDiscriminator protocol\n",
    "        step: int,\n",
    "        gamma: float\n",
    "    ) -> None:\n",
    "        super().__init__(discriminator)\n",
    "        self.__step = step\n",
    "        self.__gamma = gamma\n",
    "        self.__scheduler = StepLR(self._discriminator.optimizer, step_size = step, gamma = gamma)\n",
    "    @override\n",
    "    def step[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        loss = super().step(condition, data, target)\n",
    "        self.__scheduler.step()\n",
    "        return loss\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        super().restart()\n",
    "        self.__scheduler = StepLR(self._discriminator.optimizer, step_size = self.__step, gamma = self.__gamma)\n",
    "\n",
    "discriminator = ScheduledDiscriminator(\n",
    "    IrisDiscriminator(conditions, dim, [10, 10, 5], 0.01),\n",
    "    step = 100,\n",
    "    gamma = 0.5\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5282],\n",
      "        [0.5740],\n",
      "        [0.6081],\n",
      "        [0.6148],\n",
      "        [0.5783],\n",
      "        [0.5783],\n",
      "        [0.6172],\n",
      "        [0.5294]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5523]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Use a `StandardDiscriminator` to ease model creation\n",
    "\n",
    "from torch.nn import LeakyReLU\n",
    "\n",
    "from modugant.discriminators.standard import StandardDiscriminator\n",
    "\n",
    "\n",
    "class ReLUDiscriminator[C: int, D: int](StandardDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, _: Sequential(\n",
    "                Linear(inputs, outputs),\n",
    "                ReLU(),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "        )\n",
    "        self.__lr = lr\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "\n",
    "\n",
    "class LeakyDiscriminator[C: int, D: int](StandardDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        slope: float = 0.01,\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, _: Sequential(\n",
    "                Linear(inputs, outputs),\n",
    "                LeakyReLU(slope),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "        )\n",
    "        self.__lr = lr\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "\n",
    "discriminator = LeakyDiscriminator(\n",
    "    conditions,\n",
    "    dim,\n",
    "    steps = [10, 10, 5],\n",
    "    slope = 0.1,\n",
    "    dropout = 0.5,\n",
    "    lr = 0.001\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5967],\n",
      "        [0.5580],\n",
      "        [0.5660],\n",
      "        [0.5840],\n",
      "        [0.5643],\n",
      "        [0.5643],\n",
      "        [0.5647],\n",
      "        [0.6286]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6391]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Include some reused methods and properties into a new class to inherit from\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class EntropyDiscriminator[C: int, D: int](StandardDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        layer: Callable[[int, int, int], Module],\n",
    "        finish: Callable[[int], Module],\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(conditions, outputs, steps, layer, finish)\n",
    "        self.__lr = lr\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "\n",
    "# This allows for simpler extensions\n",
    "class ReLUDiscriminator[C: int, D: int](EntropyDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, _: Sequential(\n",
    "                Linear(inputs, outputs),\n",
    "                ReLU(),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "            lr = lr\n",
    "        )\n",
    "\n",
    "class LeakyDiscriminator[C: int, D: int](EntropyDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        slope: float = 0.01,\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, _: Sequential(\n",
    "                Linear(inputs, outputs),\n",
    "                LeakyReLU(slope),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "            lr = lr\n",
    "        )\n",
    "\n",
    "discriminator = LeakyDiscriminator(\n",
    "    conditions,\n",
    "    dim,\n",
    "    steps = [10, 10, 5],\n",
    "    slope = 0.1,\n",
    "    dropout = 0.5,\n",
    "    lr = 0.001\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6266],\n",
      "        [0.6266],\n",
      "        [0.6266],\n",
      "        [0.6266],\n",
      "        [0.6442],\n",
      "        [0.6442],\n",
      "        [0.6442],\n",
      "        [0.6442]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0.6709]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a pivoting discriminator that pivots long to wide before modeling\n",
    "# replicate predictions for each group to match one-to-one with the data\n",
    "\n",
    "class PivotDiscriminator[C: int, D: int](EntropyDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        group: int,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, layer: Sequential(\n",
    "                Linear(\n",
    "                    inputs if layer else group * inputs, # initial layer takes wide data\n",
    "                    outputs\n",
    "                ),\n",
    "                LeakyReLU(0.01),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "            lr = lr\n",
    "        )\n",
    "        self.__group = group\n",
    "    def _pivot(self, data: Tensor) -> Tensor:\n",
    "        assert data.shape[0] % self.__group == 0, \"Data must be divisible by the group size\"\n",
    "        return data.view(-1, self.__group * (self._conditions + self._outputs))\n",
    "    @override\n",
    "    def predict[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Matrix[N, One]:\n",
    "        folded = self._pivot(t_cat([condition, data], dim = 1))\n",
    "        predicted = self.forward(folded)\n",
    "        replicated = predicted.expand(-1, self.__group).reshape(-1, 1)\n",
    "        return Matrix.load(replicated, (condition.shape[0], Dim.one()))\n",
    "\n",
    "discriminator = PivotDiscriminator(\n",
    "    conditions,\n",
    "    dim,\n",
    "    group = 4,\n",
    "    steps = [10, 10, 5]\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4559],\n",
      "        [0.4409],\n",
      "        [0.4469],\n",
      "        [0.4549],\n",
      "        [0.4519],\n",
      "        [0.4556],\n",
      "        [0.4596],\n",
      "        [0.4600]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6706]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9437]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Create a Smoothness-Regularized Discriminator\n",
    "\n",
    "from torch.autograd import grad\n",
    "\n",
    "from modugant.matrix.ops import cat as m_cat\n",
    "from modugant.matrix.ops import one_hot, ones\n",
    "\n",
    "\n",
    "class SmoothDiscriminator[C: int, D: int](ExtendedDiscriminator[C, D]):\n",
    "    _discriminator: BasicDiscriminator[C, D]\n",
    "    '''\n",
    "    A discriminator that includes a smoothness penalty on the norm of the gradients\n",
    "    in the interpolated space between example cases. This ensures that the boundary\n",
    "    between real and fake data is smooth and regular.\n",
    "\n",
    "    Args:\n",
    "        discriminator: The discriminator to regularize\n",
    "        factor: The factor to multiply the smoothness penalty's contribution to loss\n",
    "\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator: BasicDiscriminator[C, D],\n",
    "        factor: float\n",
    "    ) -> None:\n",
    "        super().__init__(discriminator)\n",
    "        self.__factor = factor\n",
    "    def _blend[N: int](self, data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[N, N]:\n",
    "        # Generate interpolation points between the true and false data\n",
    "        # sample probability of assigning each original data row to each blend row\n",
    "        sample = rand((data.shape[0], data.shape[0]))\n",
    "        # sample percentages of the true cases in the blend per row\n",
    "        alpha = rand((data.shape[0], Dim.one()))\n",
    "        # sample from the true cases, use arg-max of probability to select a true into each blend row\n",
    "        # use one_hot to convert the arg-max index back into a one_hot representing rows\n",
    "        # `trues` is a [size, data.shape[0]] matrix mapping with 0/1 the sampled true data into the blend\n",
    "        # the true cases can be selected into the blend with `trues @ data`\n",
    "        trues = one_hot(\n",
    "            (sample * target.T).argmax(dim = 1, keepdim = True),\n",
    "            num_classes = target.shape[0]\n",
    "        )\n",
    "        # repeat with false cases\n",
    "        falses = one_hot(\n",
    "            (sample * (1 - target).T).argmax(dim = 1, keepdim = True),\n",
    "            num_classes = target.shape[0]\n",
    "        )\n",
    "        # use alpha/(1 - alpha) to weight the true and false cases into the blend\n",
    "        # the resulting matrix can be used as a sampler with `blend @ data`\n",
    "        return (alpha * trues) + ((1 - alpha) * falses)\n",
    "    def penalty[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        # create an interpolated data set\n",
    "        blend = self._blend(data, target)\n",
    "        ## We have to make the assumption that the discriminator is still making this transformation in its\n",
    "        ## predict method before calling .forward. See the next example below for a cleanup of this assumption.\n",
    "        inputs = m_cat(\n",
    "            (blend @ condition, blend @ data),\n",
    "            dim = 1,\n",
    "            shape = (condition.shape[0], self._conditions + self._outputs)\n",
    "        )\n",
    "        inputs.requires_grad = True\n",
    "        # feed forward the blend data\n",
    "        outputs = self._discriminator.forward(inputs)\n",
    "        # get the gradients of the output with respect to the blend data\n",
    "        gradient = grad(\n",
    "            outputs = outputs,\n",
    "            inputs = inputs,\n",
    "            grad_outputs = ones((outputs.shape[0], outputs.shape[1])),\n",
    "            create_graph = True,\n",
    "            retain_graph = True\n",
    "        )[0]\n",
    "        # calculate the norm of the gradients per row\n",
    "        norm = cast(Tensor, gradient.norm(dim = 1, keepdim = True))\n",
    "        # penalize the distance of the norm from 1\n",
    "        penalty = self.__factor * ((norm - 1) ** 2).mean(dim = None, keepdim = True)\n",
    "        return Matrix.load(penalty, shape = (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def step[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        self._discriminator.zero_grad()\n",
    "        loss = self._discriminator.loss(condition, data, target)\n",
    "        _ = loss.backward()\n",
    "        penalty = self.penalty(condition, data, target)\n",
    "        _ = penalty.backward(retain_graph = True)\n",
    "        self._discriminator.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "discriminator = SmoothDiscriminator(\n",
    "    LeakyDiscriminator(\n",
    "        conditions,\n",
    "        dim,\n",
    "        steps = [10, 10, 5],\n",
    "        slope = 0.1,\n",
    "        dropout = 0.5,\n",
    "        lr = 0.001\n",
    "    ),\n",
    "    factor = 1.0\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "penalty = discriminator.penalty(condition, data, target)\n",
    "\n",
    "print(predicted, loss, penalty, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5525],\n",
      "        [0.5525],\n",
      "        [0.5525],\n",
      "        [0.5525],\n",
      "        [0.5482],\n",
      "        [0.5482],\n",
      "        [0.5482],\n",
      "        [0.5482]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0.7508]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9783]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Add the penalty to our `FoldedDiscriminator`\n",
    "# To penalize the gradient along the folded dimension, we need to add a method to the interface\n",
    "\n",
    "# In order to access the reshaping of the PivotDiscriminator\n",
    "# we need to narrow the incoming type to depend on a method to use\n",
    "# this also allows us to bypass the assumption that the discriminator is simply concatenating the condition and data\n",
    "# For non-pivoting cases, the reshape method can be such a concatenation or otherwise based on implementation\n",
    "class ReshapeDiscriminator[C: int, D: int](BasicDiscriminator[C, D]):\n",
    "    '''Discriminator protocol class which can be smoothness regularized.'''\n",
    "\n",
    "    def __init__(self, conditions: C, outputs: D) -> None:\n",
    "        super().__init__(conditions, outputs)\n",
    "\n",
    "    def reshape[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Tensor:\n",
    "        '''\n",
    "        Reshape the incoming condition data to fit the discriminator's first layer.\n",
    "\n",
    "        Args:\n",
    "            condition (torch.Tensor): The conditional data.\n",
    "            data (torch.Tensor): The data to reshape.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The reshaped data.\n",
    "\n",
    "        '''\n",
    "        ...\n",
    "\n",
    "# rename and re-parameterize our previously protected _pivot to an exposed reshape method to match the Protocol above\n",
    "class PivotDiscriminator[C: int, D: int](EntropyDiscriminator[C, D], ReshapeDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        group: int,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, layer: Sequential(\n",
    "                Linear(\n",
    "                    inputs if layer else group * inputs,\n",
    "                    outputs\n",
    "                ),\n",
    "                LeakyReLU(0.01),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1), Sigmoid()),\n",
    "            lr = lr\n",
    "        )\n",
    "        self.__group = group\n",
    "    @override\n",
    "    def reshape[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Tensor:\n",
    "        assert data.shape[0] % self.__group == 0, \"Data must be divisible by the group size\"\n",
    "        joined = t_cat([condition, data], dim = 1) # join the condition and data\n",
    "        return joined.view(-1, self.__group * (self._conditions + self._outputs))\n",
    "    @override\n",
    "    def predict[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Matrix[N, One]:\n",
    "        folded = self.reshape(condition, data) # call the new reshape method\n",
    "        predicted = self.forward(folded)\n",
    "        replicated = predicted.expand(-1, self.__group).reshape(-1, 1)\n",
    "        return Matrix.load(replicated, (condition.shape[0], Dim.one()))\n",
    "\n",
    "# expect the underlying discriminator to be a ReshapedDiscriminator\n",
    "class SmoothDiscriminator[C: int, D: int](ExtendedDiscriminator[C, D]):\n",
    "    _discriminator: ReshapeDiscriminator[C, D]\n",
    "    '''\n",
    "    A discriminator that includes a smoothness penalty on the norm of the gradients\n",
    "    in the interpolated space between example cases. This ensures that the boundary\n",
    "    between real and fake data is smooth and regular.\n",
    "\n",
    "    Args:\n",
    "        discriminator: The discriminator to regularize\n",
    "        factor: The factor to multiply the smoothness penalty's contribution to loss\n",
    "\n",
    "    '''\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator: ReshapeDiscriminator[C, D],\n",
    "        factor: float\n",
    "    ) -> None:\n",
    "        super().__init__(discriminator)\n",
    "        self.__factor = factor\n",
    "    def _blend[N: int](self, data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[N, N]:\n",
    "        sample = rand((data.shape[0], data.shape[0]))\n",
    "        alpha = rand((data.shape[0], Dim.one()))\n",
    "        trues = one_hot(\n",
    "            (sample * target.T).argmax(dim = 1, keepdim = True),\n",
    "            num_classes = target.shape[0]\n",
    "        )\n",
    "        falses = one_hot(\n",
    "            (sample * (1 - target).T).argmax(dim = 1, keepdim = True),\n",
    "            num_classes = target.shape[0]\n",
    "        )\n",
    "        return (alpha * trues) + ((1 - alpha) * falses)\n",
    "    def penalty[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        blend = self._blend(data, target)\n",
    "        inputs = self._discriminator.reshape(\n",
    "            blend @ condition,\n",
    "            blend @ data\n",
    "        )\n",
    "        inputs.requires_grad = True\n",
    "        outputs = self._discriminator.forward(inputs)\n",
    "        gradient = grad(\n",
    "            outputs = outputs,\n",
    "            inputs = inputs,\n",
    "            grad_outputs = ones((outputs.shape[0], outputs.shape[1])),\n",
    "            create_graph = True,\n",
    "            retain_graph = True\n",
    "        )[0]\n",
    "        # pivot the gradient to the same dimensionality as the data would be\n",
    "        norm = cast(Tensor, gradient.norm(dim = 1, keepdim = True))\n",
    "        penalty = self.__factor * ((norm - 1) ** 2).mean(dim = None, keepdim = True)\n",
    "        return Matrix.load(penalty, shape = (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def step[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        self._discriminator.zero_grad()\n",
    "        loss = self._discriminator.loss(condition, data, target)\n",
    "        _ = loss.backward()\n",
    "        penalty = self.penalty(condition, data, target)\n",
    "        _ = penalty.backward(retain_graph = True)\n",
    "        self._discriminator.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "discriminator = SmoothDiscriminator(\n",
    "    PivotDiscriminator(\n",
    "        conditions,\n",
    "        dim,\n",
    "        group = 4,\n",
    "        steps = [10, 10, 5]\n",
    "    ),\n",
    "    factor = 1.0\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "penalty = discriminator.penalty(condition, data, target)\n",
    "\n",
    "print(predicted, loss, penalty, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1352],\n",
      "        [0.1352],\n",
      "        [0.1352],\n",
      "        [0.1352],\n",
      "        [0.0689],\n",
      "        [0.0689],\n",
      "        [0.0689],\n",
      "        [0.0689]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[-0.4749]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.9480]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Remove the final Sigmoid, and the log from the loss calculation\n",
    "\n",
    "# remove the inheritance from the EntropyDiscriminator\n",
    "class PivotDiscriminator[C: int, D: int](StandardDiscriminator[C, D], ReshapeDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        group: int,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda inputs, outputs, layer: Sequential(\n",
    "                Linear(\n",
    "                    inputs if layer else group * inputs,\n",
    "                    outputs\n",
    "                ),\n",
    "                LeakyReLU(0.01),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda inputs: Sequential(Linear(inputs, 1)), # remove the Sigmoid layer\n",
    "        )\n",
    "        self.__group = group\n",
    "        self.__lr = lr\n",
    "        self._optimizer = Adam(self.parameters(), lr = lr)\n",
    "    @override\n",
    "    def reshape[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Tensor:\n",
    "        assert data.shape[0] % self.__group == 0, \"Data must be divisible by the group size\"\n",
    "        joined = t_cat([condition, data], dim = 1)\n",
    "        return joined.view(-1, self.__group * (self._conditions + self._outputs))\n",
    "    @override\n",
    "    def predict[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Matrix[N, One]:\n",
    "        folded = self.reshape(condition, data)\n",
    "        predicted = self.forward(folded)\n",
    "        replicated = predicted.expand(-1, self.__group).reshape(-1, 1)\n",
    "        return Matrix.load(replicated, (condition.shape[0], Dim.one()))\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        predicted = self.predict(condition, data)\n",
    "        # remove the log from the loss calculation\n",
    "        loss = - (target.t() @ predicted + (1 - target.t()) @ (1 - predicted)) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(self.parameters(), lr = self.__lr)\n",
    "\n",
    "discriminator = SmoothDiscriminator(\n",
    "    PivotDiscriminator(\n",
    "        conditions,\n",
    "        dim,\n",
    "        group = 4,\n",
    "        steps = [10, 10, 5]\n",
    "    ),\n",
    "    factor = 1.0\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "penalty = discriminator.penalty(condition, data, target)\n",
    "\n",
    "print(predicted, loss, penalty, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2857],\n",
      "        [-0.2857],\n",
      "        [-0.2857],\n",
      "        [-0.2857],\n",
      "        [-0.4035],\n",
      "        [-0.4035],\n",
      "        [-0.4035],\n",
      "        [-0.4035]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[0.]], grad_fn=<DivBackward0>)\n",
      "tensor([[0.6550]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We have recreated the library-included `FoldedDiscriminator` and `SmoothedDiscriminator`\n",
    "\n",
    "from modugant.discriminators import FoldedDiscriminator, SmoothedDiscriminator\n",
    "\n",
    "discriminator = SmoothedDiscriminator(\n",
    "    FoldedDiscriminator(\n",
    "        conditions,\n",
    "        dim,\n",
    "        group = 4,\n",
    "        steps = [10, 10, 5]\n",
    "    ),\n",
    "    factor = 1.0\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "penalty = discriminator.penalty(condition, data, target)\n",
    "\n",
    "print(predicted, loss, penalty, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate SphereDiscriminators\n",
    "\n",
    "# Create a Module where input weights are maintained on a unit hyper-sphere\n",
    "\n",
    "from torch import no_grad\n",
    "\n",
    "\n",
    "class SphLinear(Linear):\n",
    "    @staticmethod\n",
    "    def project(network: Linear) -> None:\n",
    "        # project the weights of the network onto the unit sphere\n",
    "        with no_grad():\n",
    "            norm = cast(Tensor, network.weight.norm(2, dim = 1, keepdim = True))\n",
    "            _ = network.weight.div_(norm)\n",
    "    def __init__(self, inputs: int, outputs: int):\n",
    "        super().__init__(inputs, outputs)\n",
    "        SphLinear.project(self)\n",
    "    def reproject(self) -> None:\n",
    "        SphLinear.project(self)\n",
    "    @override\n",
    "    def reset_parameters(self) -> None:\n",
    "        super().reset_parameters()\n",
    "        SphLinear.project(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5572],\n",
      "        [0.5528],\n",
      "        [0.5538],\n",
      "        [0.6612],\n",
      "        [0.6343],\n",
      "        [0.5630],\n",
      "        [0.5638],\n",
      "        [0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6521]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Use this type of layer in a discriminator\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from modugant.discriminators import ReshapingDiscriminator\n",
    "from modugant.matrix import Matrix\n",
    "from modugant.matrix.dim import One\n",
    "\n",
    "\n",
    "## Make this a ReshapedDiscriminator, so that it can be used with our Regularizer\n",
    "## Just perform a concatenation in the reshape (identity-like)\n",
    "class SphereDiscriminator[C: int, D: int](StandardDiscriminator[C, D], ReshapingDiscriminator[C, D]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditions: C,\n",
    "        outputs: D,\n",
    "        steps: list[int],\n",
    "        dropout: float = 0.2,\n",
    "        lr: float = 0.001,\n",
    "        betas: Tuple[float, float] = (0.5, 0.9),\n",
    "        decay: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            conditions,\n",
    "            outputs,\n",
    "            steps,\n",
    "            layer = lambda ins, outs, _: Sequential(\n",
    "                SphLinear(ins, outs), # use the SphLinear layer\n",
    "                ReLU(),\n",
    "                Dropout(dropout)\n",
    "            ),\n",
    "            finish = lambda ins: Sequential(SphLinear(ins, 1), Sigmoid())\n",
    "        )\n",
    "        self.__lr = lr\n",
    "        self.__decay = decay\n",
    "        self.__betas = betas\n",
    "        self._optimizer = Adam(\n",
    "            self.parameters(),\n",
    "            lr = lr,\n",
    "            betas = betas,\n",
    "            weight_decay = decay,\n",
    "        )\n",
    "    @override\n",
    "    def reshape[N: int](self, condition: Matrix[N, C], data: Matrix[N, D]) -> Tensor:\n",
    "        # no transformation, just a concatenation\n",
    "        joined = t_cat([condition, data], dim = 1)\n",
    "        return joined\n",
    "    @override\n",
    "    def unshape[N: int](self, data: Tensor, n: N) -> Matrix[N, One]:\n",
    "        # identity\n",
    "        return Matrix.load(data, (n, Dim.one()))\n",
    "    @override\n",
    "    def loss[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        # binary cross entropy loss\n",
    "        predicted = self.predict(condition, data)\n",
    "        loss = - (target.t() @ predicted.log() + (1 - target.t()) @ (1 - predicted).log()) / len(target)\n",
    "        return Matrix.load(loss, (Dim.one(), Dim.one()))\n",
    "    @override\n",
    "    def step[N: int](self, condition: Matrix[N, C], data: Matrix[N, D], target: Matrix[N, One]) -> Matrix[One, One]:\n",
    "        loss = super().step(condition, data, target)\n",
    "        # project all linear weight vectors back onto unit sphere after updates\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, SphLinear):\n",
    "                module.reproject()\n",
    "        return loss\n",
    "    @override\n",
    "    def restart(self) -> None:\n",
    "        self._optimizer = Adam(\n",
    "            self.parameters(),\n",
    "            lr = self.__lr,\n",
    "            betas = self.__betas,\n",
    "            weight_decay = self.__decay,\n",
    "        )\n",
    "\n",
    "discriminator = SphereDiscriminator(\n",
    "    conditions,\n",
    "    dim,\n",
    "    steps = [10, 10, 5],\n",
    "    dropout = 0.2,\n",
    "    lr = 0.001,\n",
    "    betas = (0.5, 0.9),\n",
    "    decay = 0.1\n",
    ")\n",
    "\n",
    "condition = zeros((batch, conditions))\n",
    "data = randn((batch, dim))\n",
    "target = rand((batch, Dim.one())).round()\n",
    "\n",
    "predicted = discriminator.predict(condition, data)\n",
    "loss = discriminator.loss(condition, data, target)\n",
    "\n",
    "print(predicted, loss, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modugant-pr2DUMP2-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
